In this persepctive I am going to set out some personal thoughts on why I believe that creating interconnected data, and metadata, systems is generally good for the development of a healthy knowlege ecosystem. My point of view is grounded in my eperience working in scholarly publishing, and so I'm going to mainly foucs on areas of use from  on the publisher perspective. 

I recognise that what I offer up here is not the full picture, nor is it refferenced, but I hope that it can help to create a broader appreciation of some of the opportunities that we have today, and open some presecivtes on how we can collaborativly make the most of them. 

This persepctive comes from someone who is a practioneer. I have worked on buildng some of the earliest social web infratructres for science (Connotea, Nature Network), helped with building out new standards for data citation, been involved in the creation of source publishign platforms (https://github.com/elifesciences), and managed product and software development teams in this space for about twetny years. 


I'm going to start this perspective by saying that scale is good when it comes to our ability to understand the world, and at the same time operating in a coordinated way at scale is hard. This is a very broad statment so I'll take a moment to unpack it. For the first part on why scale is good? My own view on this is informed by an idea from cybernetics aobut the number of degress of freedom that we need our systems to exhibit, in order to be able to maintain stability in the face of a complex enviroment. This is Ashby's law of requsite variety, and is sometimes phrased as "in order to deal properly with the diversity of problems the world throws at you, you need to have a repertoire of responses which are (at least) as nuanced as the problems you face." Science is in the business of understanding the world, which has effectivly open ended scale, so we need to have systems that can support as diverse a set of participants as we can. As for why it is difficult to operate at scale? I want to present three types of thought experiment about how we might organsie knowlege in the world. In one scenaroio governemnts have a controlling role in manaing the systems that accredit knowlege. They set out a central agency with it's rules and regulations on what passes for knowelge. This provides a way to have strong coordiantion, and the government agency can, by diktat, modify it's own rules. If there are differnet rules stemming from different governments, then those governments can come togher under a treaty process to create aligmentm. In a way this is how the modern patent system operates, but it is a system that has difficulties in scaling, for as the number of patent applications grows, the agency will face a bottle neck on how it can process those applications. A second model might be one in which the job of accredditing knowlege is handed over to a single centralised commerical entity. In this model if that entity can generate a profit, then it could scale the resources needed operating. It can coordinate internal policy and practice, but it has the problem of not being indepednat, and only being able to allocate resources where there is a ceratin profit to be made. The third model is a distributed one, where communities of experts come together once a a minimum threashold of interested has been generated. They create their own comunity of practice, and communicate and validate knowelge claims through a shared medium - a journal. If new domains of interest emerge, a new journal can be created. If domains atrophy, flow of ideas throug those existing chanells drops accordingly. This has the advantage of scaling flexibly, but there is a significant coordination problem, as each journals works indepedantly. Given the distrubuted nature of the system, it can be quite hard to get an accurate count, but in 2020 STM estimted the number of joujrnals to have exceeded 45,000 in 2020, up from 25,000 in 2001. In this enviorment if we want to introduce a new practice (e.g. data citation), we have to wait until that practice percolates throuhg a critical volume of this set of journals, before the new practie becomes embedded, and this is what makes chaging practice at scale difficult. 




At the same time we are sometimes blind to what we can do to help create that scale, we can have a kind of a systems blindness. It takes intentional work to move us forward, and the work that the https://democratizingdata.ai  project is doing is exactly that right kind of work. So the question for us is how can we build on top of this initiative, and initiatives like this, to continue to scale our knowledge systems?


But first let me start by painting the picture of how the production of knowledge has scaled up over the last decades, at least from the perspective of the publishing corner of the ecosystem.


Data from OpenAlex (https://openalex.org/works?sort=cited_by_count%3Adesc&column=display_name,publication_year,type,open_access.is_oa,cited_by_count&group_by=publication_year) -- an open source/open data alternative to Google Scholar, tracks just a few thousand publications per year in the middle of the 19th Century. That rose to about half a million papers per year by the mid 1960s, breaking a million in 1975, and an astonishing 10 million by the end of 2020. That is a remarkable increase in scale of scholarly output since the first issue of Philosophical Transactions of the Royal Society, about 350 years ago. 

Unsurprisingly our systems have evolved over that time. How we cite, how we use indicators such as the impact factor, even peer review itself - which only evolved into its current form as late as the early 1980s, have all evolved. The business model for supporting scholary publishing is also evolving with the sift from subscriptions purchased by the library, to a transacitonal model where payment is made for the services involved in publishing an individial paper (Open Access). With this shift the payer of the servie often moves from the library to the individual researcher, or granting body. Our digital systems were built around the subscription model, and in that model we needed to privde key indicators of usage at journal level. In an Open Access model where the payer is purchasing servies around individiual aritlces we have to be able to provide data at that level. This shift moves us from a journal centric view of the world to an article-centric view. We are now trying to build systems that move us on again to a more person centered view. Being able to give an individual key information about how their articles are doing, or where the aritcles htey have reviewed are in the syste, or how data they have used in their papers gets reused by others, are the kinds of features that are common in other web applications, but remain frustratingly rare in academic journals. Moroever being able to get a good sense of the topics of interest of a researcher can help us build more robust and resiliant systems. As an example, an indicator of potential fraud is where the co-authorship network of a researcher varies wildly between each sequential paper that they publish. To be able to even analyse this reuquires us to have systems in place that build out individial profiles of researcher's interestes and activity. 


This is connected with the shift to open access where our relationship is increasingly closer to the author than the library, but it is also motivated by the increase in scale that we are seeing. Being able to get high quality data about our authors, what topics they are interested in, what data they work with, how the work that they publish has rippling effects of impact, is critical for publishers who want to help our authors understand the impact of their work, help funders understand the impact of what they have funded, and help ourselves as publishers deal with scaling issues through finding ways to bring more efficiencies into the system. 

The layer of connection between researchers, data sets, publications, and government agencies that democratising data is creating is the kind of information that we need to be able access in order to do this work well. I'll paint a picture here to explain why having agencies investing in this kind of service is good. When we think about citations, those citations are calls from one paper to another to ackknowlege the former work, but the influence of that work is not limited and defined only by the citation. For example there will be many people who read that paper who do not cite it (perhaps they do not go on to write another paper, and yet perhaps it gives them a change in perspective that changes their behaviour). There may be uses of the data from the paper, that do not result in antoher paper, but where the use is of vlaue. There may be appearances of that paper in plicy documents, but where the paper is not formally cited. We can image the full life of the research object as casting a digital shadow, and citations are just one manifestation of the life of that object. Over the last few years a number of systems have been built to try to create a better view into that abstract life of the object. https://www.altmetric.com was created to track mentioned of research papers in social media feeds, as an indicator of attention that the paper was getting. One of the prime sources for altmetrics was orginally twitter. https://www.overton.io has recently been created to text mine policy papers to create the back mapping from policy to documents, and BMJ has collaborated with Overton to create https://impactanalytics.bmj.com to provide a similalr persepctive on the impact of clinical research. These are worthy efforts, but depend either on 3rd party data systems that might not be stable in the long term (e.g. twitter), or on the continual provisioning of data mining systems from 3rd parties. These sources are less likey to remain in existence over the longer term. Having the agencies who are the soruce of data invest in creating a source of informaiton about the use of their data offers the potential for a more stable long term, and openly avialale, source of information about the broader life of reseacher objects. 




To illustrate this At BMJ alone, in 2022 we had over 770 million access requests to the content that we publish. It remains that case that most of these access requests were anonymous, and that we have not yet been able to create the story behind those numbers to provide to our authors to start to be able to connect our authors with the audiences for their work. We are in the process of building out the systems that will allow us to do just that, and we are seeking ways to turn what we do from being transactional around dumb documents into being more active around not only the papers we publish, but the underlying concepts, entities, and audiences that that are connected through the work and the people developing the work. 

Benefits that we will see are around being able to scale this way should allow us to allocate resources more effectively. From a publishing perspective  today one of the hardest things to do is to find the right reviewer for the right paper. Needing to cycle through multiple reviewers is a waste in the system. In addition when we apply any level of selection criteria to a journal, e.g. based on scope or community interest, there will be papers that get rejected from one journal to go on and get published elsewhere. This is again a form or waste that better data could help us reduce. 

Better data could also help us create better connections across communities or disciplines of practice. The holy grail in medical research is to get from bench to bed, where fundamental research can be applied in medical practice, but that requires the ability to connect across multiple boundaries of practice.  If we can scale insight around who is doing what, with what kinds of data, we can make it easier to connect researchers who are tackling common problems. More exciting still is finding ways to connect disparate and different people where cross pollination of ideas can lead to real innovation.  

Where we have had a role in connecting folk there have been outsized impacts from making those connections. BMJ has had over 31,000 people coming together over the course of 2022 in BMJ-led events and we have been able to see the percolation of ideas from these events into hospital trusts.  

These kinds of opportunities for scale, whilst interesting in themselves, are in service of something larger. For BMJ that is about creating a healthier world. We know in the future that rising and ageing populations will put healthcare systems under increasing pressure. This increases the pressure to find ways to equip our health care professionally with more effective ways to train and work.  In reality many of us do work that we hope can move the needle on problems, be those local, national or global in scale.  

All of these issues of scale, are however, predicated on the need to have a trustworthy scholarly record - that we can trust what is being published. That assumption is coming under increased strain.

As mentioned already, one of the reasons behind that is that we have not scaled how our systems work in the same way that we have scaled the amount of research activity that is happening globally. 

We are seeing an increase in the submission of fake papers, and the creation of fake researchers, and fake peer review rings. While knowledge is created and transmitted by people, the scholarly record is the idempotent record of state over time. 

We need that record to be trustworthy, and we need new tools and techniques to deal with new patterns of misbehaviour. More highly validated and trustworthy data about research outputs is critical here.

Publishers are thinking hard about these issues. Springer Nature have made investments of about  US$293.7 million in technology between 2019 and 2021 "to improve and speed up the publishing experience, ensure research integrity is protected, and misinformation is tackled", and there is a collective effort to create tools to identify fake papers. 

We are adopting more and better linking between entities such as grant identifiers, authors, papers, funders, data sets, and institutions, but we are only ever able to link what kinds of data we have available. 

The more high quality data that we have that we can link together, the more robust our picture of research at scale becomes, and the more capable we are of addressing the issues and opportunities I've laid out above. With that in mind I would love to see the democratisingdata.ai project explicity expose identifiers such as ORCID records, and FundRef records. What they have done is begin to create the bridge bewteen governamenatl data sets, and the publications that mention those data sets. The publication identiofiers can open a further bride to other identifiers that couold create a richer tapestry of connections. 

A question that might be interesting to consider is whether data sets could be considered as socical objects, along the lines that we once thought when thinking about the archiecture of Web 2.0 sites. In this framing sites that becaome popular that involved social sharing, could be thought of as having some core soical object. For instagram that social object is the photograph, for spotify the social object is the song, and for facebook the social object is the friend relationship. Could we see initiatives such as democratisingdatabase.ai as turing governemntal data sets into social objects, and through aggrating information about their use, creating communities of interest that break across disciplinary silos? For publishers this is probably less directy interesting as norms of behaviour within specific journals tends to be more conservative than the reading beahviour of researchers.  

To get data directly from government agencies, or funders, about what data is being used, by whom, in what way, is invaluable to broaden out the picture that I am presenting here. 

The more actors in our knowledge economy move to adopt data driven and open data practices, the better it is for everyone. 

There is clearly a strong appetite to improve our systems in ways that improve the scholarly record, but it's not sufficient to build something, and hope that it will get adopted. 

Any change to a system at scale incurs cost, and requires work and  coordinated effort to get taken up by a broad range of folk. As a simple example, updating all of our publishing platforms to support an updated version of a data citation standard requires adoption by over 3500 publishers. There are some centralised vendors, but it still reuqires each of those publishers to consider their polcies adopt, update their terms and conditions, update their submissions gudielines, and the pubslihign infrastructre that they operate. 

In order for change to happen it has to align with the incentives that organisations want to follow. That means either solving pain points that have become unbearable (and believe me, existing systems can bear quite a lot of pain before they are forced to undergo change), or more powerfully make it easier for organisations to realise opportunities by reducing friction for those organisations.  

Publishers are increasingly focused on real world impact. BMJ for example has invested in the development of BMJ Impact Analytics which uses NLP to extract references to academic papers from policy documents, helping to understand that research to impact connection. As with many other publishers we are also 
Committed to supporting the UN sustainable development goals. This is because we know that this is what most of our customers want from us. 

Impact can often be an amorphous, and excellence can often be fethisised, so where it is important to connect data to an impact narratives where we can do so. For example BMJ has invested in the development of BMJ Impact Analytics which uses NLP to extract references to academic papers from policy documents, helping to understand that research to impact connection. This can provide a measure of how frequencly ceratin topics make it into policy. PLOS have developed a set of Open Science Indicators which "identify and quantify instances of specific Open Science practices" (https://theplosblog.plos.org/2023/10/open-science-indicators-q2-2023/). The Chan Zuckerberg Initative have invested in the creation of mentions of software in academic papers (https://medium.com/czi-technology/new-data-reveals-the-hidden-impact-of-open-source-in-science-11cc4a16fea2). While it is easy to imagine the abstrct graph of connections of research objects, and their interactions with different steps in the research process, each of the above iniatives has requried time, investment, and will need some level of sustainabily model if these signals are to be generated over some duration of time. For a few years many initiatives built on top of the Microsfot Academic Graph, but that resource was retired. At that point the Allen Institute ingested some of that open data into Semantic Scholoar, but did not keep the graph updated. OpenAlex has stepped in to create an ongoing resource, which is fantastic for now, but the number of organsiasions that are working on making this kind of infomraiton available remain fairly small. Having access to informaiton about use of real data sets fits into these frameworks well, and the work of Democratising Data is squarely aligned with this. Moreover, having a variety of kinds of institution participating increases the likelihood that we may find a sustainable way of making all of this work, through diversifying effort. 


It is helpful to start with a specific domain, as you can get leverage in that domain initially. Having direct proof of value is invaluable in making the case. 

It is helpful to identify who the activist academic, funders, and commercial partners are, give them a voice, and let them share their experiences. 

Allow them to create and share stories of success, for while the data is a necessary condition, alone is not sufficient, we need stories and narratives to move us. 




I want to end with the following note. This enterprise of scientific creation of knowledge might seem well embedded in our societies, but it has only been around for a few hundred years, a blink of an eye. Our use of the internet, and internet scale technologies might seem fixed, but they are newer still. Devices and techniques that can work at scale are only now becoming more commonplace within our research infrastructures. The future stretches out in front of us, and is there to be shaped by us, for us. Creating good ways of scaling collaboration, uncoupling systems, but finding common connections through identifiers, and having trustworthy provenance of that data, are all things that can help us to create a far more resilient, scalable, and better research future, and are all things that we can collectivly participate in creating. I've been working on many of these kinds of systems over the last twenty years, and I'm a little surprised to still find myself working on this. I did think it might be soloved easily, but making the data avialble remains a key issue, so I'm really excited to see this kind of thinking percolate into government, and I'm looking forward to the next twenty years of work ahead. 

We have to acknowlege what the potential downsides that a program like this can entail, and what we must do to operate in a way that is respecticl and valuable to the overall research ecosystem. By building on a wide network of data avialble we can increase our abilty to automate things like marketing messages, requests for review, requets for submissions to journals. Any one of these activities represents a call on the attention of the researcher. The question we have to anser is whether we can provide recommendations to the researcher that are ultimatly of more value to them than not. The risk is that these systems become net consumers of attention without adding any efficnecy to researchers, but eqully the opportunity to use data to focus attention and build new connections is manifest. While the impact factor is largely derided, what it does represent is a proxy for attention. One of the values that high imact journals have is the abilgty to focus attention. Being able to use better information on a more democtaric basis, we should be able to direct attention in a more equitable way than the one lmiited dimesion of attention that is operating today. Another key aspect of how we operate systems like safely and respectfully is this is that we have to be transparent about what we are doing, and work through approriate consent frameworks. That we have researchers at the heart of our journals is necessary condition for us to be able to make these kinds of systems in a way that works for the community. 

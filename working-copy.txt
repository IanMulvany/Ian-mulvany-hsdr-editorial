I'm going to start this perspective by saying that scale is always good when it comes to our ability to understand the world. While getting to scale is hard, we hold it in our hands to increase the ability of the systems we work in to scale. 

At the same time we are sometimes blind to what we can do to help create that scale, we can have a kind of a systems blindness. It takes intentional work to move us forward, and the work that the https://democratizingdata.ai  project is doing is exactly that right kind of work. So the question for us is how can we build on top of this initiative, and initiatives like this, to continue to scale our knowledge systems?

But first let me start by painting the picture of how the production of knowledge has scaled up over the last decades, at least from the perspective of the publishing corner of the ecosystem.


Data from OpenAlex (https://openalex.org/works?sort=cited_by_count%3Adesc&column=display_name,publication_year,type,open_access.is_oa,cited_by_count&group_by=publication_year) -- an open source/open data alternative to Google Scholar, tracks just a few thousand publications per year in the middle of the 19th Century. That rose to about half a million papers per year by the mid 1960s, breaking a million in 1975, and an astonishing 10 million by the end of 2020. That is a remarkable increase in scale of scholarly output since the first issue of Philosophical Transactions of the Royal Society, about 350 years ago. 

Unsurprisingly our systems have evolved over that time. How we cite, how we use indicators such as the impact factor, even peer review itself - which only evolved into its current form as late as the early 1980s, have all evolved. The business model for supporting scholary publishing is also evolving with the sift from subscriptions purchased by the library, to a transacitonal model where payment is made for the services involved in publishing an individial paper (Open Access). With this shift the payer of the servie often moves from the library to the individual researcher, or granting body. Our digital systems were built around the subscription model, and in that model we needed to privde key indicators of usage at journal level. In an Open Access model where the payer is purchasing servies around individiual aritlces we have to be able to provide data at that level. This shift moves us from a journal centric view of the world to an article-centric view. We are now trying to build systems that move us on again to a more person centered view. Being able to give an individual key information about how their articles are doing, or where the aritcles htey have reviewed are in the syste, or how data they have used in their papers gets reused by others, are the kinds of features that are common in other web applications, but remain frustratingly rare in academic journals. Moroever being able to get a good sense of the topics of interest of a researcher can help us build more robust and resiliant systems. As an example, an indicator of potential fraud is where the co-authorship network of a researcher varies wildly between each sequential paper that they publish. To be able to even analyse this reuquires us to have systems in place that build out individial profiles of researhcer interestes and activity. 

Our systems have evolved over time, how we cite, how we use indicators such as the impact factor, even peer review itself - which only evolved into its current form as late as the early 1980s, but as publishers the key shift that we are undergoing now is moving from a document centric view of the world to a person centric view of the world. 

This is connected with the shift to open access where our relationship is increasingly closer to the author than the library, but it is also motivated by the increase in scale that we are seeing. Being able to get high quality data about our authors, what topics they are interested in, what data they work with, how the work that they publish has rippling effects of impact, is critical for publishers who want to help our authors understand the impact of their work, help funders understand the impact of what they have funded, and help ourselves as publishers deal with scaling issues through finding ways to bring more efficiencies into the system. 

The layer of connection between researchers, data sets, publications, and government agencies that democratising data is creating is exactly the kind of information that we need to be able to do this work well. 

To illustrate this At BMJ alone, in 2022 we had over 770 million access requests to the content that we publish. It remains that case that most of these access requests were anonymous, and that we have not yet been able to create the story behind those numbers to provide to our authors to start to be able to connect our authors with the audiences for their work. We are in the process of building out the systems that will allow us to do just that, and we are seeking ways to turn what we do from being transactional around dumb documents into being more active around not only the papers we publish, but the underlying concepts, entities, and audiences that that are connected through the work and the people developing the work. 

Benefits that we will see are around being able to scale this way should allow us to allocate resources more effectively. From a publishing perspective  today one of the hardest things to do is to find the right reviewer for the right paper. Needing to cycle through multiple reviewers is a waste in the system. In addition when we apply any level of selection criteria to a journal, e.g. based on scope or community interest, there will be papers that get rejected from one journal to go on and get published elsewhere. This is again a form or waste that better data could help us reduce. 

Better data could also help us create better connections across communities or disciplines of practice. The holy grail in medical research is to get from bench to bed, where fundamental research can be applied in medical practice, but that requires the ability to connect across multiple boundaries of practice.  If we can scale insight around who is doing what, with what kinds of data, we can make it easier to connect researchers who are tackling common problems. More exciting still is finding ways to connect disparate and different people where cross pollination of ideas can lead to real innovation.  

Where we have had a role in connecting folk there have been outsized impacts from making those connections. BMJ has had over 31,000 people coming together over the course of 2022 in BMJ-led events and we have been able to see the percolation of ideas from these events into hospital trusts.  

These kinds of opportunities for scale, whilst interesting in themselves, are in service of something larger. For BMJ that is about creating a healthier world. We know in the future that rising and ageing populations will put healthcare systems under increasing pressure. This increases the pressure to find ways to equip our health care professionally with more effective ways to train and work.  In reality many of us do work that we hope can move the needle on problems, be those local, national or global in scale.  

All of these issues of scale, are however, predicated on the need to have a trustworthy scholarly record - that we can trust what is being published. That assumption is coming under increased strain.

As mentioned already, one of the reasons behind that is that we have not scaled how our systems work in the same way that we have scaled the amount of research activity that is happening globally. 

We are seeing an increase in the submission of fake papers, and the creation of fake researchers, and fake peer review rings. While knowledge is created and transmitted by people, the scholarly record is the idempotent record of state over time. 

We need that record to be trustworthy, and we need new tools and techniques to deal with new patterns of misbehaviour. More highly validated and trustworthy data about research outputs is critical here.

Publishers are thinking hard about these issues. Springer Nature have made investments of about  US$293.7 million in technology between 2019 and 2021 "to improve and speed up the publishing experience, ensure research integrity is protected, and misinformation is tackled", and there is a collective effort to create tools to identify fake papers. 

We are adopting more and better linking between entities such as grant identifiers, authors, papers, funders, data sets, and institutions, but we are only ever able to link what kinds of data we have available. 

The more high quality data that we have that we can link together, the more robust our picture of research at scale becomes, and the more capable we are of addressing the issues and opportunities I've laid out above. With that in mind I would love to see the democratisingdata.ai project explicity expose identifiers such as ORCID records, and FundRef records. What they have done is begin to create the bridge bewteen governamenatl data sets, and the publications that mention those data sets. The publication identiofiers can open a further bride to other identifiers that couold create a richer tapestry of connections. 

A question that might be interesting to consider is whether data sets could be considered as socical objects, along the lines that we once thought when thinking about the archiecture of Web 2.0 sites. In this framing sites that becaome popular that involved social sharing, could be thought of as having some core soical object. For instagram that social object is the photograph, for spotify the social object is the song, and for facebook the social object is the friend relationship. Could we see initiatives such as democratisingdatabase.ai as turing governemntal data sets into social objects, and through aggrating information about their use, creating communities of interest that break across disciplinary silos? For publishers this is probably less directy interesting as norms of behaviour within specific journals tends to be more conservative than the reading beahviour of researchers.  

To get data directly from government agencies, or funders, about what data is being used, by whom, in what way, is invaluable to broaden out the picture that I am presenting here. 

The more actors in our knowledge economy move to adopt data driven and open data practices, the better it is for everyone. 

There is clearly a strong appetite to improve our systems in ways that improve the scholarly record, but it's not sufficient to build something, and hope that it will get adopted. 

Any change to a system incurs cost, and requires work and  coordinated effort to get taken up by a broad range of folk. 

In order for change to happen it has to align with the incentives that organisations want to follow. That means either solving pain points that have become unbearable (and believe me, existing systems can bear quite a lot of pain before they are forced to undergo change), or more powerfully make it easier for organisations to realise opportunities by reducing friction for those organisations.  

Publishers are increasingly focused on real world impact. BMJ for example has invested in the development of BMJ Impact Analytics which uses NLP to extract references to academic papers from policy documents, helping to understand that research to impact connection. As with many other publishers we are also 
Committed to supporting the UN sustainable development goals. This is because we know that this is what most of our customers want from us. 

The work of Democratising Data is squarely aligned with your needs, but how can we best activate that?

The following are some rules of thumb that I hope can help. 

Some strategies for consideration by this community. 
It is important to make the licensing of working with this technology or data easy. Open licences, and clear terms and conditions, are key to allow legal review on collaboration to move smoothly. It is helpful to start with a specific domain, as you can get leverage in that domain initially. Having direct proof of value is invaluable in making the case. It is helpful to identify who the activist academic, funders, and commercial partners are, give them a voice, and let them share their experiences. Allow them to create and share stories of success, for while the data is a necessary condition, alone is not sufficient, we need stories and narratives to move us. 

This enterprise of scientific creation of knowledge might seem well embedded in our societies, but it has only been around for a few hundred years, a blink of an eye. Our use of the internet, and internet scale technologies might seem fixed, but they are newer still, and devices and techniques that can work at scale are only now becoming more commonplace within our research infrastructures. The future stretches out in front of us, and is there to be shaped by us, for us. Creating good ways of scaling collaboration, uncoupling systems, but finding common connections through identifiers, and having trustworthy provenance of that data, are all things that can help us to create a far more resilient, scalable, and better research future. 

We have to acknowlege what the potential downsides that a program like this can entail, and what we must do to operate in a way that is respecticl and valuable to the overall research ecosystem. By building on a wide network of data avialble we can increase our abilty to automate things like marketing messages, requests for review, requets for submissions to journals. Any one of these activities represents a call on the attention of the researcher. The question we have to anser is whether we can provide recommendations to the researcher that are ultimatly of more value to them than not. The risk is that these systems become net consumers of attention without adding any efficnecy to researchers, but eqully the opportunity to use data to focus attention and build new connections is manifest. While the impact factor is largely derided, what it does represent is a proxy for attention. One of the values that high imact journals have is the abilgty to focus attention. Being able to use better information on a more democtaric basis, we should be able to direct attention in a more equitable way than the one lmiited dimesion of attention that is operating today. Another key aspect of how we operate systems like safely and respectfully is this is that we have to be transparent about what we are doing, and work through approriate consent frameworks. That we have researchers at the heart of our journals is necessary condition for us to be able to make these kinds of systems in a way that works for the community. 

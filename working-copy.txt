In this perspective, I am going to outline some personal thoughts on why I believe that creating interconnected data and metadata systems is generally beneficial for the development of a robust knowledge ecosystem. My viewpoint is grounded in my experience working in scholarly publishing, so I'm going to mainly focus on areas of use from the publisher's perspective.

I acknowledge that what I present here doesn't provide a complete picture, nor is it referenced, but I hope that it can help to foster a broader appreciation of some of the opportunities we have today and open some perspectives on how we can collaboratively maximize them.

This perspective comes from someone who is a practitioner. I have worked on building some of the earliest social web infrastructures for science (Connotea, Nature Network), contributed to the establishment of new standards for data citation, been involved in the creation of source publishing platforms (https://github.com/elifesciences), and managed product and software development teams in this arena for about twenty years.

I'll begin by stating that scale is good when it comes to our ability to understand the world, yet operating in a coordinated way at scale is challenging. This is a broad statement, so I'll take a moment to unpack it. In terms of why scale is good, my perspective is informed by a concept from cybernetics about the degrees of freedom our systems need to exhibit to maintain stability in a complex environment. This is Ashby's law of requisite variety and is often phrased as: "To adequately deal with the diversity of problems the world presents, you need a repertoire of responses that are (at least) as nuanced as the problems you face." Science aims to understand the world, which is essentially open-ended in scale, so we need systems that support as diverse a set of participants as we can. But why is it difficult to operate at scale? I want to present three thought experiments about organizing knowledge in the world.

In one scenario, governments have a controlling role in managing the systems that accredit knowledge. They lay out a central agency with its rules and regulations on what constitutes knowledge. This allows for strong coordination, and the government agency can modify its own rules. If different rules sprout from different governments, then they can come together under a treaty process to create alignment. Conceptually, this is how the modern patent system operates, but it struggles with scalability. As the number of patent applications grows, the agency faces a bottleneck in processing those applications.

The second model might involve a single centralized commercial entity accrediting knowledge. If this entity can generate a profit, it has the potential to scale the required resources for operation. It can coordinate internal policy and practice but comes with the problem of not being independent and only able to allocate resources where profits can be made.

The third model is a distributed one, where communities of experts gather once a minimum threshold of interest has been generated. They create their community of practice and validate knowledge claims through a shared medium - a journal. The advantage of this model is the flexibility of scalability, but it presents a significant coordination problem, as each journal operates independently. By its distributed nature, it can be challenging to get an accurate count, but in 2020, STM estimated the number of journals to have exceeded 45,000, up from 25,000 in 2001. In this environment, if we want to introduce a new practice (e.g., data citation), we have to wait until that practice permeates a critical volume of this set of journals before the new practice becomes embedded, making changing practices at scale difficult.

Simultaneously, we sometimes overlook what we can do to help create that scale, an issue I term as systems blindness. It takes intentional work to make progress, and the work that the https://democratizingdata.ai project is doing is a prime example of this. So, the question for us is, how can we build on top of this initiative, and initiatives like this, to continually scale our knowledge systems?

First, let me illustrate how the production of knowledge has ramped up over the past few decades, particularly from the publishing corner of the ecosystem.

OpenAlex's data (https://openalex.org/works?sort=cited_by_count%3Adesc&column=display_name,publication_year,type,open_access.is_oa,cited_by_count&group_by=publication_year), an open-source/open data alternative to Google Scholar, showed only a handful of publications per year in the mid-19th Century. That number increased to about half a million papers per year by the mid-1960s, exceeded a million in 1975, and an astonishing 10 million by the end of 2020. This is a remarkable increase in the scale of scholarly output since the first issue of the Philosophical Transactions of the Royal Society about 350 years ago.

Unsurprisingly, our systems have evolved over that period. Practices like citing, using impact factor indicators, and even peer review, which only assumed its current form in the early 1980s, have all evolved. The business model supporting scholarly publishing is also evolving, transitioning from library-subscription-based to a transactional model where payment is made for services involved in publishing an individual paper (Open Access). With this shift, the payer often transfers from the library to the individual researcher or granting body. Our digital systems were built around the subscription model, which allowed us to provide key usage indicators at the journal level. In an Open Access model, wherein the payer purchases services around individual articles, we must provide data at that level. Consequently, we are now trying to build systems that shift our perspective from journal-centric to person-centered. Being able to give an individual information about the performance of their articles, the status of articles they have reviewed, or data they have used in their papers reused by others, are features common in other web applications but remain frustratingly rare in academic journals. Furthermore, having a solid understanding of a researcher's interest can help us build more robust and resilient systems. For instance, an indicator of potential fraud is when a researcher's co-authorship network varies wildly between sequential papers they publish. To analyze this requires systems that develop individual profiles of researchers' interests and activities.

This shift correlates with the transition to open access, where our relationship grows closer with the author rather than the library. It's also motivated by the increase in scale we are witnessing. Having high-quality data about our authors, their areas of interest, the data they work with, and the ripple effect of the work they publish is critical for publishers looking to help authors understand the impact of their work, aid funders in understanding the influence of what they have funded, and address scaling issues by finding ways to bring more efficiencies into the system.

The layer of connection between researchers, datasets, publications, and government agencies that the democratizing data project is creating is the kind of information we need to access to do this work effectively. To illustrate why investing in this kind of service is beneficial, let's look at citations. Citations are acknowledgments from one paper to another, admitting to prior work, but their influences are not limited by the citation itself. For example, many people who read the paper do not necessarily cite it. Perhaps it changes their perspective, leading to a shift in behavior. There may be uses of the paper's data that do not result in another paper but are still valuable. There may be instances of the paper appearing in policy documents without formal citation. The full life of the research object casts a digital shadow, with citations being just one facet. Several systems have been established over the past few years to offer a better glimpse into the abstract life of these objects. Altmetric.com tracks social media mentions of research papers, and more recently, Overton.io has been developed to text mine policy documents. The information generated by such platforms is invaluable, but they depend on third-party data systems that may not provide long-term stability, making the information less likely to exist in the future. Having agencies investing in creating a resource for information about data usage offers the potential for a long-term, openly available, more stable source of information about the broader life of research objects.

In 2022 alone, BMJ received over 770 million access requests for the content we publish. Most of these requests were anonymous, and we haven't yet managed to create a story behind those numbers that we could provide to our authors to help connect them with their audiences. We are in the process of building out the systems that will enable us to do just that, and we seek ways to move beyond simply processing transactions with documents into focusing on not only the papers we publish, but also the underlying concepts, entities, and audiences connected through the work and the people developing it.

The benefits that we'll likely see from being able to scale in this manner should enable us to allocate resources more effectively. From a publishing perspective, one of the hardest things to do is finding the right reviewer for the appropriate paper. Needing to cycle through multiple reviewers is an inefficiency in the system. Additionally, when we apply selection criteria to a journal, such as the scope or community interest, there will be papers that get rejected from one journal only to be published elsewhere. This also represents a waste which better data could help us reduce.

Better data could also help us establish enhanced connections across communities or disciplines of practice. The "holy grail" in medical research is the transition from bench to bed, which involves applying fundamental research in medical practice. But bridging that gap requires the ability to connect across multiple boundaries of practice. If we can scale insights about who is doing what and with which type of data, we can make it easier to connect researchers who are solving common problems. More excitingly, we can find ways to connect disparate people, where the cross-pollination of ideas can stimulate real innovation.

Where we have facilitated connections, the impacts have been outsized. BMJ orchestrated over 31,000 people converging in BMJ-led events throughout 2022, and we've witnessed the permeation of ideas from the events into hospital trusts.

While these scaling opportunities are inherently interesting, they serve a more significant purpose. For BMJ, that purpose is promoting a healthier world. With rising and aging populations, healthcare systems will face mounting pressure. This heightens the need to develop more effective ways to train and support our healthcare professionals. Most of us aspire to make a difference in problems, be they local, national, or global in scale.

However, all these scaling issues hinge on having a trustworthy scholarly record—that we can trust in what is being published. That assumption is increasingly under strain as we have not scaled our systems to match the global growth in research activity.

Our systems are confronting a rise in fake paper submissions, fake researchers, and fake peer review rings. While knowledge is created and transmitted by people, the scholarly record is the objective, stable record over time.

That record needs to be trustworthy, and we need new tools and techniques to deal with new patterns of misconduct. Here, more thoroughly validated and trustworthy data about research outputs are vital.

Publishers are earnestly pondering these issues. Springer Nature has invested about US$293.7 million in technology between 2019 and 2021 "to improve and speed up the publishing experience, ensure research integrity is protected, and misinformation is tackled." Collectively, we are striving to create tools to identify fake papers.

We are gradually embracing more and better linking between entities such as grant identifiers, authors, papers, funders, datasets, and institutions, but we can only link the types of data we have at hand.

The more high-quality data we can link together, the more robust our overview of research at scale becomes, and the better equipped we are to address the issues and opportunities I've detailed above. To this end, I would love to see the democratizingdata.ai project explicitly expose identifiers like ORCID records and FundRef records. They've already begun bridging the gap between governmental datasets and the publications that cite those datasets. These publication identifiers could further bridge to other identifiers that could create a richer tapestry of connections.

An interesting question to consider is whether datasets could be regarded as social objects, akin to a perspective we took when contemplating the architecture of Web 2.0 sites. In this framing, popular sites involving social sharing could be thought of as having a core social object. For Instagram, that object is the photograph; for Spotify, it's the song; and for Facebook, it's the friendship. Could initiatives like democratizingdatabase.ai transform governmental datasets into social objects, and by aggregating information about their use, create interest communities that transcend disciplinary silos? For publishers, this idea may be less directly interesting as behavioral norms within specific journals tend to be more conservative than the reading behavior of researchers.

Receiving data directly from government agencies or funders about data usage, by whom, and in what manner, is invaluable for broadening the picture I'm presenting here.

The more participants in our knowledge economy adopt data-driven and open data practices, the better off everyone will be.

There's clearly a strong appetite to enhance our systems to improve the scholarly record, but merely building something and hoping for adoption is insufficient.

Any system change at the scale incurs a cost and calls for work and coordinated efforts for broad uptake. For example, updating all our publishing platforms to support an updated version of a data citation standard requires adoption by over 3,500 publishers. Some central vendors are available, but it still requires each of these publishers to reconsider their policies, update their terms and conditions, revise their submission guidelines, and upgrade the publishing infrastructure they operate.

Publishers are increasingly focused on real-world impact and have developed a set of impact reports that aim to show how our work supports better evidence, better decisions, better systems. We, along with many other publishers, are committed to supporting the UN's sustainable development goals.

Impact can often be ambiguous and excellence can often be fetishized, so it's important to connect data to impact narratives where possible. For example, BMJ has invested in the development of BMJ Impact Analytics, which uses Natural Language Processing (NLP) to extract references to academic papers from policy documents, helping to understand the research-to-impact connection. PLOS has developed a set of Open Science Indicators which "identify and quantify instances of specific Open Science practices" (https://theplosblog.plos.org/2023/10/open-science-indicators-q2-2023/). The Chan Zuckerberg Initiative has invested in the creation of software mentions in academic papers (https://medium.com/czi-technology/new-data-reveals-the-hidden-impact-of-open-source-in-science-11cc4a16fea2). Although it's easy to imagine the abstract graph of research objects' connections and their interactions with different steps in the research process, each of these initiatives has required time, investment, and some level of sustainability if these signals are to be generated over a significant duration. For a few years, many initiatives built on top of the Microsoft Academic Graph, but that resource was retired. Subsequently, the Allen Institute absorbed some of that open data into Semantic Scholar but didn't keep the graph updated. OpenAlex has filled the void to create an ongoing resource, which is fantastic for now but the numbers of organizations working on making this information available remain small. Access to information about the usage of real datasets fits well into these frameworks, and the work of Democratising Data aligns squarely with this. Moreover, having a variety of institutions participating increases the likelihood that we may find a sustainable way of making this work, through diversifying the effort.

We need to acknowledge the potential pitfalls a program like this can present and identify what steps we must take to operate in a way that is respectful and valuable to the overall research ecosystem. By building upon a vast network of available data, we can increase our ability to automate aspects like marketing messages, review requests, and journal submission requests. Each one of these activities represents a call for the researcher's attention. We have to determine if we can provide recommendations to the researcher that are ultimately more valuable to them than not. The risk is that these systems become net consumers of attention without adding any efficiency for researchers, but the opportunity to use data to focus attention and build new connections is clear. While the impact factor is mostly criticized, it represents a proxy for attention. One of the valuable aspects of high-impact journals is their ability to focus attention. By using better information on a more democratic basis, we should be able to direct attention in a more equitable way than the limited dimension of attention operating today. Another essential aspect of how systems like these operate safely and respectfully is that we must be transparent about what we're doing and work through appropriate consent frameworks. Having researchers at the heart of our journals is a necessary condition for us to create these kinds of systems in a way that works for the community.

I want to conclude with the following note. The scientific creation of knowledge may seem deeply entrenched in our societies, but it has only been around for a few hundred years—a blink of an eye. Our use of the internet and internet scale technologies may seem fixed, but they are even newer. Devices and techniques that can function at scale are only now becoming more commonplace within our research infrastructures. The future lies ahead, and we can shape it for our mutual benefit. Creating good ways of scaling collaboration, decoupling systems while finding common ground through identifiers, and ensuring trustworthy provenance of data can all help us create a more resilient, scalable, and better research future—something we can collectively help to build. I've been working on many of these types of systems over the past twenty years, and I'm surprised to still find myself engaged in this work. I thought it might be easily solved, but making the data available remains a key issue.